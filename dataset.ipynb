{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_dataset(zip_file, extract_folder):\n",
    "    # Check if the zip file exists\n",
    "    if not os.path.exists(zip_file):\n",
    "        print(f\"Error: Zip file '{zip_file}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Create the extraction folder if it doesn't exist\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "    # Open and extract the zip file\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "\n",
    "    print(f\"Dataset extracted successfully to '{extract_folder}'.\")\n",
    "\n",
    "# Example usage:\n",
    "zip_file = '/teamspace/studios/this_studio/C2Seg_AB_splitted.zip'  # Replace with your zip file path\n",
    "extract_folder = '/teamspace/studios/this_studio/dataset'  # Replace with the folder where you want to extract\n",
    "\n",
    "unzip_dataset(zip_file, extract_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio\n",
    "!pip install segmentation_models_pytorch\n",
    "!pip install -U albumentations\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # List of image file names (assuming all three directories have the same file names)\n",
    "        self.image_names = os.listdir(os.path.join(root_dir, 'msi'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the file names\n",
    "        img_name = self.image_names[idx]\n",
    "\n",
    "        # Paths to the msi, sar, and label images\n",
    "        msi_path = os.path.join(self.root_dir, 'msi', img_name)\n",
    "        sar_path = os.path.join(self.root_dir, 'sar', img_name)\n",
    "        label_path = os.path.join(self.root_dir, 'label', img_name)\n",
    "\n",
    "        # Read the msi image\n",
    "        with rasterio.open(msi_path) as msi_src:\n",
    "            msi_image = msi_src.read()  # Shape: (4, height, width)\n",
    "\n",
    "        # Normalize the msi image to [0, 1]\n",
    "        msi_image = msi_image.astype(np.float32)\n",
    "        msi_image = (msi_image - msi_image.min()) / (msi_image.max() - msi_image.min())\n",
    "\n",
    "        # Read the sar image\n",
    "        with rasterio.open(sar_path) as sar_src:\n",
    "            sar_image = sar_src.read()  # Shape: (2, height, width)\n",
    "\n",
    "        # Normalize the sar image to [0, 1]\n",
    "        sar_image = sar_image.astype(np.float32)\n",
    "        sar_image = (sar_image - sar_image.min()) / (sar_image.max() - sar_image.min())\n",
    "\n",
    "        # Concatenate msi and sar images along the first dimension\n",
    "        combined_image = np.concatenate((msi_image, sar_image), axis=0)  # Shape: (6, height, width)\n",
    "\n",
    "        # Read the label image\n",
    "        with rasterio.open(label_path) as label_src:\n",
    "            label_image = label_src.read(1)  # Read the first band. Shape: (height, width)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        combined_image = torch.tensor(combined_image, dtype=torch.float32)\n",
    "        label_image = torch.tensor(label_image, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            combined_image, label_image = self.transform((combined_image, label_image))\n",
    "\n",
    "        return combined_image, label_image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Custom transform function\n",
    "class CustomTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),  # Set probability to 0.5\n",
    "            T.RandomVerticalFlip(p=0.5),    # Set probability to 0.5\n",
    "            # T.RandomRotation(degrees=30),  # Uncomment if you want to use rotation\n",
    "        ])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        # Apply the same transformation to the image and the label\n",
    "        seed = np.random.randint(2147483647)  # Make a seed with numpy generator\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.transform(image)\n",
    "        torch.manual_seed(seed)\n",
    "        label = self.transform(label.unsqueeze(0)).squeeze(0)  # Unsqueeze and squeeze to keep label shape\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "\n",
    "train_root_dir = '/teamspace/studios/this_studio/dataset/C2Seg_AB_splitted/train'\n",
    "test_root_dir = '/teamspace/studios/this_studio/dataset/C2Seg_AB_splitted/test'\n",
    "val_root_dir = '/teamspace/studios/this_studio/dataset/C2Seg_AB_splitted/val'\n",
    "\n",
    "train_dataset = CustomDataset(root_dir=train_root_dir)\n",
    "test_dataset = CustomDataset(root_dir=test_root_dir)\n",
    "val_dataset = CustomDataset(root_dir=val_root_dir)\n",
    "\n",
    "# You can now pass this custom dataset to a DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for i, (images, labels) in enumerate(val_dataloader):\n",
    "    print(images.shape, labels.shape)\n",
    "    print(labels.unique())\n",
    "    # Process your batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
